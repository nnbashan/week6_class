{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0dGmZqBIupP"
   },
   "source": [
    "# CIVE 5699 - Programming & Data Science in CEE\n",
    "```\n",
    "Week 6 - Data Acquisition and Management\n",
    "Dr. Matthew J. Eckelman\n",
    "TA Nail Bashan\n",
    "```\n",
    "\n",
    "Contents:\n",
    "* Data Formatting\n",
    "* Data Cleaning\n",
    "* Data Curation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRLNUBskIupS"
   },
   "source": [
    "## Data cleaning and formatting\n",
    "\n",
    "In this tutorial we start with a sample air quality data provided by [PurpleAir](https://www2.purpleair.com/) for 5 different locations in Boston. Each sensor provides temperature (°F), relative humidity (%) and particulate matter concentrations (μg/m3) from **two channels** (cfa and cfb). Timestamps are given in **UTC** (usually 5 hours ahead of Boston) and raw data is given in .csv format, with data files found in the class folder on Canvas\n",
    "\n",
    "Let's start by importing required libraries and reading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F34Yu9dTcv8j"
   },
   "outputs": [],
   "source": [
    "# Pandas and numpy for data storage and manipulation\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(zip([1,2,3],['a','b','c']),columns=['column1','column2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Z-rb5lUc_-G"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array([1,2,3,4]),np.array([1,2,3,4])*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiI1spemIupU"
   },
   "outputs": [],
   "source": [
    "# tqdm will provide us a progress bar, which can be helpful while running long loops\n",
    "from tqdm import tqdm\n",
    "\n",
    "for k in tqdm(range(1000)):\n",
    "  weather = 'cold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88ggAD22dIeC"
   },
   "outputs": [],
   "source": [
    "# Glob and os is used to interact with the database while reading/writing, especially helpful when we have big data sourced from multiple files\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# matplot for plotting scatter/line plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# filter warnings so we won't get annoying package update notifications\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "\n",
    "# timedelta is used to manipulate datetime formats\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqYCHPqJIupW"
   },
   "outputs": [],
   "source": [
    "# In this example we only have 5 .csv files but this code could handle any number, much faster than uploading manually one by one!\n",
    "# First we read all the file names in sensor_data folder ending with '.csv'\n",
    "file_names = glob.glob(\"sensor_data/*.csv\")\n",
    "\n",
    "# and initiate an empty list for these .csv folders, an empty dataframe to put them all together\n",
    "csv_list = []\n",
    "df = pd.DataFrame()\n",
    "\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mFKTvY8d65z"
   },
   "outputs": [],
   "source": [
    "# Create a loop to read all these file names into a dataframe (sensor_readings) and aggregate all 5 sensor_readings into the main dataframe (df)\n",
    "\n",
    "for file in tqdm(file_names):\n",
    "    sensor_readings = pd.read_csv(file)\n",
    "\n",
    "    # .csv files came with an extra column 'Unnamed: 0' we don't need that so we can drop it\n",
    "    sensor_readings = sensor_readings.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "    # our file names are recorded as 'sensor#.csv' we can use name[:-4] format to not consider last 4 characters (.csv)\n",
    "    # take our sensor name and make it a column in sensor_readings dataframe\n",
    "    sensor_readings['ids'] = [os.path.basename(file)[:-4]] * len(sensor_readings)\n",
    "\n",
    "    #append to the main dataframe\n",
    "    df = df.append(sensor_readings)\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.drop('Unnamed: 0.1',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5CnfC-iIupY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Our data looks like this, let's check boxplots to identify possible outliers\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8F9KErIIupY"
   },
   "outputs": [],
   "source": [
    "# We can see that there are some extreme temperature readings (up to 2000°F)\n",
    "\n",
    "df[['humidity','temperature']].boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kY33yWbGIupZ"
   },
   "outputs": [],
   "source": [
    "# Similarly, particulate matter readings also reflect some outliers\n",
    "\n",
    "df[['cfa','cfb']].boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9bx660gIupZ"
   },
   "outputs": [],
   "source": [
    "# Also there are some cases where two channels were not in agreement under the same conditions\n",
    "plt.scatter(df.cfa,df.cfb)\n",
    "\n",
    "# We can see this better with comparing a x=y line\n",
    "x = range(int(max(df.cfa)))\n",
    "y = range(int(max(df.cfa)))\n",
    "plt.plot(x, y,color='red', label='x=y line')\n",
    "\n",
    "plt.xlabel('cfa readings')\n",
    "plt.ylabel('cfb readings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qB-HMDgIupa"
   },
   "source": [
    "Under this conditions we need to follow a data cleaning procedure before any modeling or analysis. For data cleaning and formatting we will follow the steps below:\n",
    "\n",
    "```\n",
    "1. Delete missing data (missing value for both channels).\n",
    "2. Delete observations with data from only one channel.\n",
    "3. Delete data with abnormal temperature (T) and relative humidity (RH) readgins. (T<-200°F, or T>10000°F; RH>100%, or RH<0%).\n",
    "4. Delete data if the difference between two channels is larger than 5 μg/m3 or △>61%. (△ = ((cfa-cfb)*2) / (cfa+cfb) * 100)\n",
    "\n",
    "```\n",
    "Barkjohn, Karoline K., Brett Gantt, and Andrea L. Clements. \"Development and application of a United States-wide correction for PM 2.5 data collected with the PurpleAir sensor.\" Atmospheric Measurement Techniques 14.6 (2021): 4617-4637."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gThXBW_0Iupb"
   },
   "outputs": [],
   "source": [
    "# Step 1. and 2.\n",
    "\n",
    "# As can be seen there are multiple missing data points (for example in Temperature column)\n",
    "df.sort_values('temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cdS_Yf7hT9G"
   },
   "outputs": [],
   "source": [
    "# How can we drop these rows (sensor readings) with missing values? How many readings with missing data points we have?\n",
    "# Python documentation will (mostly) have all the answers you want, so visit https://pandas.pydata.org/docs/\n",
    "\n",
    "beforedrop = len(df)\n",
    "\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7vbhbwzIupd"
   },
   "outputs": [],
   "source": [
    "# Step 3.\n",
    "# How can we drop these rows (recordings) with abnormal temperature and humidity readings? How many recordings with abnormal data points we have?\n",
    "\n",
    "###\n",
    "\n",
    "rhDeleted = ##\n",
    "tDeleted = ##\n",
    "\n",
    "print(f'{rhDeleted} data points are deleted due to abnormal humidity')\n",
    "print(f'{tDeleted} data points are deleted due to abnormal humidity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uqd58e-ZIupe"
   },
   "outputs": [],
   "source": [
    "# Step 4.\n",
    "\n",
    "# To avoid division by 0, we will replace all zeros with a small value\n",
    "\n",
    "df[['cfa','cfb']] = df[['cfa','cfb']].replace(0, 0.0001)\n",
    "grouppedA = np.array(df.cfa)\n",
    "grouppedB = np.array(df.cfb)\n",
    "\n",
    "condition1= ##\n",
    "condition2 = ##\n",
    "\n",
    "\n",
    "deleted_c1c2 = ##\n",
    "\n",
    "print(f'{deleted_c1c2} data points are deleted due to disagreement between channels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9lbN9eYIupe"
   },
   "outputs": [],
   "source": [
    "# Finally, lets convert UTC timestamps to EDT\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# EDT is 4 hours behind UTC in May, its time to use the 'timedelta' function\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IX5Aam_ZIupf"
   },
   "outputs": [],
   "source": [
    "# and see how is the daily variation of PM2.5 in Boston from these 5 locations\n",
    "# Get the hours as a seperate column first:\n",
    "\n",
    "df['hour'] = pd.DatetimeIndex(df['date']).hour\n",
    "\n",
    "# and groupby hour with hourly median readings\n",
    "\n",
    "plt.plot(df.groupby('hour').median().cfa)\n",
    "plt.xlabel('hour')\n",
    "plt.ylabel('PM2.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jTOrLwPIupf"
   },
   "source": [
    "\n",
    "## Data curation and formatting\n",
    "\n",
    "\n",
    "One way to access available data is through APIs (Application Programming Interfaces). APIs act as bridges between different software systems, facilitating seamless communication and data sharing, thereby simplifying integration and the utilization of their functionalities. In this tutorial, we will access 2019 US Census Data and prepare it for use in our applications.\n",
    "\n",
    "### Preparation\n",
    "\n",
    "APIs typically require a registered key, which may be publicly available or require purchasing a paid service. For example, the US Census Bureau offers a free API service, so the first step would be to register for your own key.\n",
    "\n",
    "```\n",
    "https://api.census.gov/data/key_signup.html\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmJRiEK3Iupf"
   },
   "outputs": [],
   "source": [
    "api_key = '--'\n",
    "## change this with your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0V5B_kGdIupg"
   },
   "outputs": [],
   "source": [
    "# requests library helps Python interact with web servers, JSON is the data format we are using\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON, or JavaScript Object Notation, is a simple and readable way to organize and exchange data, using a format that resembles key-value pairs and lists. JSON has become a de facto standard for data interchange on the web. Example:\n",
    "```\n",
    "{\n",
    "  \"name\": \"Jon Snow\",\n",
    "  \"age\": 24,\n",
    "  \"house\": Targaryen,\n",
    "  \"title\": \"Lord Commander\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xS_NE_ZYIupg"
   },
   "source": [
    "We are interested in counties of Greater Boston area (Middlesex, Norfolk, Suffolk and Essex counties) in Massachusetts. We will start by setting our initial parameters from the API's documentation: https://www2.census.gov/geo/pdfs/maps-data/data/tiger/tiger2006se/app_a03.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2gsyy0fIupg"
   },
   "outputs": [],
   "source": [
    "ma_state_code = '25'\n",
    "\n",
    "## Can you find codes for Middlesex, Norfolk, Suffolk and Essex counties?\n",
    "\n",
    "county_codes = [###]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CS0Cg-_Iuph"
   },
   "source": [
    "From this api we can collect over 10000 variables, but today suppose we just want to find median household income, total population, and white population percentage. Required keys for these variables can be found in: https://api.census.gov/data/2019/acs/acs1/variables.html\n",
    "\n",
    "```\n",
    "B03002_003E : Total white population\n",
    "B03002_001E : Total population\n",
    "B19013_001E : Median household income\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fx93VszvIuph"
   },
   "outputs": [],
   "source": [
    "# Let's add one more, can you see point out any variables you are interested?\n",
    "\n",
    "variables = 'B03002_001E,B03002_003E,B19013_001E, ####'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bq8x53mLIuph"
   },
   "outputs": [],
   "source": [
    "# We start by creating an empty dataframe, and iterate over all counties to send API requests.\n",
    "\n",
    "demographicdata=pd.DataFrame({})\n",
    "\n",
    "for county in county_codes:\n",
    "  # the format for sending this request will be available in APIs' website, this is the format that census.gov uses:\n",
    "    url='https://api.census.gov/data/2019/acs/acs5?get='+variables+'&for=tract:*&in=state:'+ma_state_code+'%20county:'+county+'&key='+api_key\n",
    "    # we send a request to connect this url\n",
    "    response = requests.request(\"GET\", url)\n",
    "    # load all the data (in json format) as a text\n",
    "    a=json.loads(response.text)\n",
    "    # and read in into a temporary (temp) dataframe\n",
    "    temp=pd.DataFrame(a)\n",
    "    # finally combine with our main dataframe\n",
    "    demographicdata=pd.concat([demographicdata,temp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3FKlA6GmIuph"
   },
   "outputs": [],
   "source": [
    "# We should format this data before using\n",
    "\n",
    "demographicdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "BjSSI1RGIupi"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (1396565662.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [21]\u001b[0;36m\u001b[0m\n\u001b[0;31m    demographicdata = demographicdata.reset_index(drop=True)\u001b[0m\n\u001b[0m                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "# Assign first row to column names, and change column names to variables we are using (ex. 'B03002_001E':'total_pop',)\n",
    "\n",
    "\n",
    "demographicdata=demographicdata.rename(columns=demographicdata.iloc[0]).drop(demographicdata.index[0])\n",
    "demographicdata=demographicdata.rename(###)\n",
    "\n",
    "demographicdata = demographicdata.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvaHKInmIupi"
   },
   "outputs": [],
   "source": [
    "# finally, lets calculate white population percentages in each census tract\n",
    "# data is recorded as string so we should change it to integer before performing a division\n",
    "\n",
    "demographicdata['white_perc'] = ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7FwKLUCIupi"
   },
   "outputs": [],
   "source": [
    "demographicdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data curation and formatting 2\n",
    "\n",
    "In this tutorial, we will access OpenStreetMap API and prepare it for use in our applications.\n",
    "\n",
    "\n",
    "OpenStreetMap [OSM](https://www.openstreetmap.org) is a collaborative, open-access mapping project that allows users to create, edit, and share detailed maps and geographic data globally. OpenStreetMap provides users with access to a wide range of Points of Interest (POIs) and allows them to store geographical data. [Overpass API](https://overpass-turbo.eu/) is widely used for data acquisition from OpenStreetMap. [Wiki.openstreetmap](https://wiki.openstreetmap.org/wiki/Map_features) has all the information you need for building a query.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import overpy\n",
    "import pandas as pd\n",
    "\n",
    "# Define the area we want to search (min_x, min_y, max_x, max_y)\n",
    "bbox = \"42.335736164968644, -71.09337538026332,42.343543696646826, -71.08362188693184\"\n",
    "api = overpy.Overpass()\n",
    "\n",
    "# Send a query to API\n",
    "query = f\"\"\"\n",
    "[out:json];\n",
    "(\n",
    "    node[\"amenity\"=\"fast_food\"]({bbox});\n",
    "\n",
    ");\n",
    "out body;\n",
    "\"\"\"\n",
    "\n",
    "# Store the results\n",
    "result = api.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an empty dictionary\n",
    "restaurants_dict = {}\n",
    "\n",
    "for node in result.nodes:\n",
    "    # Extract relevant information (e.g., name) from each node\n",
    "    cuisine = node.tags.get(\"cuisine\", \"N/A\")\n",
    "    center = (float(node.lon), float(node.lat))\n",
    "    # Add information to the dictionary\n",
    "    restaurants_dict[node.tags.get(\"name\", \"N/A\")] = {\n",
    "        \"cuisine\": cuisine,\n",
    "        \"center\": center,\n",
    "        # Add more fields as needed\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>center</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Subway</th>\n",
       "      <td>sandwich</td>\n",
       "      <td>(-71.0905736, 42.3366147)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Papa John's</th>\n",
       "      <td>pizza</td>\n",
       "      <td>(-71.0840056, 42.337504)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kigo Kitchen</th>\n",
       "      <td>asian</td>\n",
       "      <td>(-71.0875733, 42.3390735)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Popeyes</th>\n",
       "      <td>chicken</td>\n",
       "      <td>(-71.0875401, 42.3390448)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sweet Tomatoes Pizza</th>\n",
       "      <td>pizza</td>\n",
       "      <td>(-71.0876747, 42.3391846)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Burger 373</th>\n",
       "      <td>burger</td>\n",
       "      <td>(-71.0876089, 42.3391112)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chaat House</th>\n",
       "      <td>american</td>\n",
       "      <td>(-71.0875008, 42.3390053)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Five Guys</th>\n",
       "      <td>burger</td>\n",
       "      <td>(-71.0860734, 42.3421493)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dunkin'</th>\n",
       "      <td>donut;coffee_shop</td>\n",
       "      <td>(-71.0892211, 42.3365119)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qdoba</th>\n",
       "      <td>mexican</td>\n",
       "      <td>(-71.0908434, 42.3397769)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wings Over Boston</th>\n",
       "      <td>wings</td>\n",
       "      <td>(-71.0879034, 42.3409203)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Panera Bread</th>\n",
       "      <td>sandwich;bakery</td>\n",
       "      <td>(-71.0866173, 42.3417162)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Churchill's</th>\n",
       "      <td>sandwich</td>\n",
       "      <td>(-71.0886658, 42.3388645)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tu Taco</th>\n",
       "      <td>mexican</td>\n",
       "      <td>(-71.0876385, 42.3391463)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poke Station</th>\n",
       "      <td>N/A</td>\n",
       "      <td>(-71.0874787, 42.3410957)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Playa Bowls</th>\n",
       "      <td>açaí</td>\n",
       "      <td>(-71.0862469, 42.3420087)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                cuisine                     center\n",
       "Subway                         sandwich  (-71.0905736, 42.3366147)\n",
       "Papa John's                       pizza   (-71.0840056, 42.337504)\n",
       "Kigo Kitchen                      asian  (-71.0875733, 42.3390735)\n",
       "Popeyes                         chicken  (-71.0875401, 42.3390448)\n",
       "Sweet Tomatoes Pizza              pizza  (-71.0876747, 42.3391846)\n",
       "Burger 373                       burger  (-71.0876089, 42.3391112)\n",
       "Chaat House                    american  (-71.0875008, 42.3390053)\n",
       "Five Guys                        burger  (-71.0860734, 42.3421493)\n",
       "Dunkin'               donut;coffee_shop  (-71.0892211, 42.3365119)\n",
       "Qdoba                           mexican  (-71.0908434, 42.3397769)\n",
       "Wings Over Boston                 wings  (-71.0879034, 42.3409203)\n",
       "Panera Bread            sandwich;bakery  (-71.0866173, 42.3417162)\n",
       "Churchill's                    sandwich  (-71.0886658, 42.3388645)\n",
       "Tu Taco                         mexican  (-71.0876385, 42.3391463)\n",
       "Poke Station                        N/A  (-71.0874787, 42.3410957)\n",
       "Playa Bowls                        açaí  (-71.0862469, 42.3420087)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also store data as a dataframe\n",
    "\n",
    "pd.DataFrame.from_dict(restaurants_dict, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Additional exercise:\n",
    "Can you determine the number of convenience stores in the area within these boundaries?\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
